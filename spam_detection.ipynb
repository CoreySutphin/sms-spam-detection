{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boxed-retirement",
   "metadata": {},
   "source": [
    "# SMS Spam Detection\n",
    "\n",
    "The goal of this notebook is to explore a dataset of SMS messages, where these messages are either tagged as `spam` or `ham`(legitimate messages). We will build a predictive model to detect whether a certain text is a spam message.\n",
    "\n",
    "The general layout of this notebook is as follows:\n",
    "\n",
    "1. Imports and loading data\n",
    "2. Exploring the text\n",
    "3. Pre-processing\n",
    "4. Turning the raw text into feature vectors(Using BERT)\n",
    "5. Splitting the data into training/testing set\n",
    "6. Utilizing a simple logistic regression model with the feature vectors\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are leveraging the SMS Spam dataset provided here -> https://www.kaggle.com/uciml/sms-spam-collection-dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-hacker",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "variable-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import statistics\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "starting-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 419 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from en-core-web-sm==3.0.0) (3.0.3)\n",
      "Requirement already satisfied: pathy in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: setuptools in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (52.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.58.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.3)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download the SpaCy \"small\" model if it is not already downloaded\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "designed-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a trained SpaCy pipeline to pre-process text\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-mouth",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Here we are going to dig through the text and try to understand the shape of the text we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mathematical-trademark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"input/spam.csv\")[[\"v1\", \"v2\"]]\n",
    "data.columns = [\"label\", \"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-evening",
   "metadata": {},
   "source": [
    "Let's see the distribution of labels within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proud-hollow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-radar",
   "metadata": {},
   "source": [
    "So in total we have:\n",
    "\n",
    "* 5,572 texts\n",
    "* 4,825(~83%) of texts are legitimate(`ham`)\n",
    "* 747(~13%) of texts are `spam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "increasing-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do we have any empty texts with no content in the dataset?\n",
    "data[data[\"text\"] == \"\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unavailable-cassette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"text\"].isnull()].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-price",
   "metadata": {},
   "source": [
    "So no null or missing values, are there any strange characters in the texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "located-chassis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           []\n",
       "1           []\n",
       "2       [', ']\n",
       "3           []\n",
       "4          [']\n",
       "         ...  \n",
       "5567       [�]\n",
       "5568       [�]\n",
       "5569        []\n",
       "5570       [']\n",
       "5571        []\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].apply(lambda x: re.findall(r'[^\\w\\s,.\\(\\)&!?*-]', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-rebound",
   "metadata": {},
   "source": [
    "Looking through the texts, it seems all emoji's have already been replaced with the � character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-tackle",
   "metadata": {},
   "source": [
    "Check the average length of each type message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "technical-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_data = data[data[\"label\"] == \"ham\"]\n",
    "spam_data = data[data[\"label\"] == \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "spoken-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of Ham texts is: 71.02134715025906\n",
      "Average length of Spam texts is: 138.429718875502\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the average length of ham vs. spam messages.\n",
    "average_ham_length = statistics.mean(ham_data[\"text\"].apply(lambda x: len(x)))\n",
    "average_spam_length = statistics.mean(spam_data[\"text\"].apply(lambda x: len(x)))\n",
    "\n",
    "print(f\"Average length of Ham texts is: {average_ham_length}\")\n",
    "print(f\"Average length of Spam texts is: {average_spam_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-standard",
   "metadata": {},
   "source": [
    "So spam messages do appear to be significantly longer on average. The spammer is trying to elicit some response from the user rather than having an actual conversation, so this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-toolbox",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Here we list some processing functions to parse the raw text and get it ready for further analytics/processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "legislative-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace all numbers with the symbol \"#\"\n",
    "    text = re.sub(\"\\d+\", \"#\", text)\n",
    "    \n",
    "    # Replace long sequences of whitespace with a single whitespace\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "        \n",
    "    # Using SpaCy, remove stopwords and lemmatize each word\n",
    "    doc = nlp(text)\n",
    "    text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    \n",
    "    # Now return the processed text\n",
    "    return text\n",
    "\n",
    "def process_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Pre-process a dataframe by adding in some descriptive columns, and pre-processing the text.\"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df[\"text\"] = new_df[\"text\"].apply(lambda x: process_text(x))\n",
    "    new_df[\"text_length\"] = new_df[\"text\"].apply(lambda x: len(x))\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incomplete-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "referenced-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point , crazy .. available bugis n grea...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar ... joke wif u oni ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry # wkly comp win fa cup final tkts #...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun early hor ... u c ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think go usf , live</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  text_length\n",
       "0   ham  jurong point , crazy .. available bugis n grea...           92\n",
       "1   ham                      ok lar ... joke wif u oni ...           29\n",
       "2  spam  free entry # wkly comp win fa cup final tkts #...          113\n",
       "3   ham                        u dun early hor ... u c ...           27\n",
       "4   ham                            nah think go usf , live           23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-oklahoma",
   "metadata": {},
   "source": [
    "## Text Representation\n",
    "\n",
    "When feeding text into some statistical model for prediction or for extracting insights, we must transform the text into some representation that the model will be able to understand. This is typically a vector of numbers used to describe a specific token, sentence, or document.\n",
    "\n",
    "These vectors can be generated with hand-picked features, or they can be generated from pre-trained language models like word2vec or more recently BERT. These pre-trained models take a sentence as input and output a numeric value for each token that represents that word in the sentence. These representations have empirically established state-of-the-art results on most NLP tasks, proving themselves generally superior to hand-picked and curated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-foster",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "This model utilizes a pre-trained BERT model to generate powerful vector representations of the sentences in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hazardous-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "BERT_MODEL_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load in the pre-trained tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL_TYPE)\n",
    "model = DistilBertModel.from_pretrained(BERT_MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "certified-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the pre-processed text using BERT's \"word-piece\" tokenizer\n",
    "# This turns text into a list of IDs, where each ID maps a token to a particular embedding in BERT's\n",
    "# pre-trained embeddings table.\n",
    "processed_df[\"encoded_text\"] = processed_df[\"text\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "starting-chaos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 18414, 17583, 2391, 1010, 4689, 1012, 10...\n",
       "1       [101, 7929, 2474, 2099, 1012, 1012, 1012, 8257...\n",
       "2       [101, 2489, 4443, 1001, 1059, 2243, 2135, 4012...\n",
       "3       [101, 1057, 24654, 2220, 7570, 2099, 1012, 101...\n",
       "4       [101, 20976, 2228, 2175, 2149, 2546, 1010, 244...\n",
       "                              ...                        \n",
       "5567    [101, 1001, 1050, 2094, 2051, 3046, 1001, 3967...\n",
       "5568    [101, 1035, 1038, 2175, 9686, 24759, 5162, 320...\n",
       "5569    [101, 12063, 1010, 1008, 6888, 1012, 1012, 101...\n",
       "5570    [101, 3124, 7743, 2075, 2552, 2066, 4699, 4965...\n",
       "5571                 [101, 20996, 10258, 1012, 2995, 102]\n",
       "Name: encoded_text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"encoded_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-lemon",
   "metadata": {},
   "source": [
    "When working with BERT, all vectors need to be of the same length. We can simply get the maximum length of a sequence in our dataset, and pad all other sequences to that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "found-class",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(processed_df[\"text_length\"])\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-longitude",
   "metadata": {},
   "source": [
    "BERT was designed to only be able to process 512 tokens from text at a time. Since the average text length is so much higher than this max length, we are making the assumption that we can simply take the first 512 tokens from each sequence. This means we will pad each sequence to 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "first-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each sequence to 512 tokens(using 0 as the padding token)\n",
    "PADDING_TOKEN = 0\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "processed_df[\"encoded_text\"] = processed_df[\"encoded_text\"].apply(\n",
    "    lambda x: x[:512] + [PADDING_TOKEN for i in range(len(x), 512)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-delicious",
   "metadata": {},
   "source": [
    "Now the magic, we pass in these padded sequences through the pre-trained BERT model, and extract the last hidden layer. This hidden layer provides a feature vector for each token, and a feature vector for the sentence. For the purposes of spam detection(which is sentence classification), we will only extract the sentence-level feature vector, and discard the token-level feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passing-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 512)\n",
      "(5572, 512)\n"
     ]
    }
   ],
   "source": [
    "encoded_text = np.array([i for i in processed_df[\"encoded_text\"].values])\n",
    "\n",
    "# Construct a \"mask\", basically telling BERT to ignore the extra padding we have added\n",
    "attention_mask = np.where(encoded_text != 0, 1, 0)\n",
    "\n",
    "print(encoded_text.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "living-lawrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 43min 2s, sys: 14min 39s, total: 1h 57min 42s\n",
      "Wall time: 20min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = torch.tensor(encoded_text)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\"\"\" \n",
    "    Due to resource limitations(writing this to run on a CPU versus a dedicated GPU machine),\n",
    "    we are batching the input into the model. NOTE that this could take up to 20 minutes to finish on a \n",
    "    12 core machine with 16GB RAM.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 100\n",
    "features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(input_ids), BATCH_SIZE):\n",
    "        last_hidden_states = model(input_ids[i:i + BATCH_SIZE], attention_mask=attention_mask[i:i + BATCH_SIZE])\n",
    "        # Extracting the sentence-level feature vector from the model output\n",
    "        batch_features = last_hidden_states[0][:,0,:].numpy()\n",
    "        if not len(features):\n",
    "            features = np.copy(batch_features)\n",
    "        else:\n",
    "            features = np.concatenate((features, batch_features), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "accessible-malawi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-pollution",
   "metadata": {},
   "source": [
    "Because it took 20 minutes to generate these feature vectors, let's go ahead and write them out to disk so we can iterate faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "short-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bert-features.npy', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-survey",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Here we split our data into a training/testing set. Normally you would also have a vlidation set used to tweak hyperparamaters of your model. Due to time-constraints we will be doing a simplye 80/20 training-test split, with an equal proprotion of `ham` and `spam` messages in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "public-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(features, processed_df[\"label\"], test_size=0.2, random_state=42, stratify=processed_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "interim-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 768)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "relative-keyboard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115, 768)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "physical-welcome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3859\n",
       "spam     598\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "outer-skiing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     966\n",
       "spam    149\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-airline",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We are going to use our feature vectors obtained from BERT and use a tried-and-true logistic regression model to see what kind of results we get with this learned input representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "graduate-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corey/.virtualenvs/sms-spam-t44czJzN/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9901345291479821"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_clf = LogisticRegression()\n",
    "logreg_clf.fit(train_data, train_labels)\n",
    "\n",
    "# Now that we've fit our model, evaluate on the test daata\n",
    "logreg_clf.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-stuart",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This was a simple study on spam detection, but the key player here was the informative feature vectors we were able to automatically extract using a pre-trained deep learning model(BERT). While we lose the interpretibility of the features, we gain an empiricaly powerful representation of our texts based on a robust deep-learning architecture trained on terabytes of English text.\n",
    "\n",
    "## Results\n",
    "\n",
    "We achieve an accuracy of 99%, meaning we were able to correctly classify 99% of texts in the test set as either `ham` or `spam`. While with most machine-learning problems this high of an accuracy would indicate a coding mistake or extreme over-fitting, in the cases of simple spam detection it is actually not unusual at all. If you look at the notebooks on Kaggle for this problem most people seem to be achieving 97-99% accuracy on this dataset. This could be a result of spam messages rarely mimicking real conversations, and thus being easy to separate out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-mouth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
